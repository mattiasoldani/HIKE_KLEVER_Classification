{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92df3fca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LogNorm, LinearSegmentedColormap\n",
    "\n",
    "# sklearn, general tools\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, \\\n",
    "    roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, ConfusionMatrixDisplay, \\\n",
    "    precision_recall_curve, roc_curve, plot_roc_curve\n",
    "\n",
    "# sklearn, models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fbbcba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# open data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57772d5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# misc settings\n",
    "\n",
    "classlabel = {  # class labels and their names\n",
    "    \"k-2pi\" : 0,\n",
    "    \"k-pinunu\" : 1,  # signal\n",
    "    \"lambda-pin\" : 5,\n",
    "    \"background\" : 99,  # all the channels but k-pinunu\n",
    "}\n",
    "\n",
    "bTossFaultyVertex = True  # toss faulty vertex data?\n",
    "\n",
    "limLambdaWeights = [1e-8, 100]  # range limits to the lambda weights; set lower to 0 to keep all\n",
    "\n",
    "bOnlyInFV = False  # limit the whole analysis to the FV?\n",
    "shift_fv = 150  # shift between absolute and relative (in data) position of the FV\n",
    "fv_edges = (280, 350)  # FV boundaries, in absolute longitudinal coordinate\n",
    "bOnlyInPtCut = False  # limit the whole analysis to within the cut on the pion p_t?\n",
    "pt_cut = 0.140  # lower cut on the pion p_t\n",
    "\n",
    "bBinaryClass = True  # if True (False), only two classes chosen below (all the classes) are kept\n",
    "whichsig = \"k-pinunu\"  # class to be used as signal? (set regardless of value of bBinaryClass)\n",
    "whichbkg = \"lambda-pin\"  # class to be used as background? (if bBinaryClass is False, overwritten to \"background\")\n",
    "nEvsPop = 100000  # (max.) nr. of events per class - if needed: set None to skip\n",
    "nMultLambda = 1  # nMultLambda*nEvsPop lambda or tot. backgroud background events will be loaded (at max.)\n",
    "\n",
    "bReweightStat = False  # reweight each class according to expected statistics? (if False, normalise each class to 1)\n",
    "nb = 5*200*3000  # nr. of bursts in the whole experiment lifetime\n",
    "ppb = 2e13  # nr. of protons on target per burst\n",
    "kpp = 2.1e-5  # nr. of kaons per proton on target\n",
    "beamcorr = (0.256/0.4)**2  # ratio between beam solid angles for short and long tunnel\n",
    "lambdacorr = (37/73)*(2.00/2.19)  # correction to the kaon production yield for lambda\n",
    "classflux = {  # expected flux of parent particles out of the target\n",
    "    \"k-2pi\" : nb*ppb*kpp*beamcorr,\n",
    "    \"k-pinunu\" : nb*ppb*kpp*beamcorr,\n",
    "    \"lambda-pin\" : nb*ppb*kpp*beamcorr*lambdacorr,\n",
    "}\n",
    "classbr = {  # BR of each decay class\n",
    "    \"k-2pi\" : 0.864e-3,\n",
    "    \"k-pinunu\" : 3.0e-11,\n",
    "    \"lambda-pin\" : 0.358,\n",
    "}\n",
    "#classflux = {  # expected flux of parent particles out of the target\n",
    "#    \"k-2pi\" : 0.446e12,\n",
    "#    \"k-pinunu\" : 15483,\n",
    "#    \"lambda-pin\" : 8.22e13,\n",
    "#}\n",
    "#classbr = {  # BR of each decay class\n",
    "#    \"k-2pi\" : 1,\n",
    "#    \"k-pinunu\" : 1,\n",
    "#    \"lambda-pin\" : 1,\n",
    "#}\n",
    "\n",
    "classngen = {  # zOptical initial events per class\n",
    "    \"k-2pi\" : 7e9,\n",
    "    \"k-pinunu\" : 70e6,\n",
    "    \"lambda-pin\" : 100e6,\n",
    "}\n",
    "\n",
    "split_test_fraction = 0.5  # fraction of total statistics to be used for testing\n",
    "\n",
    "confusionmatrix_norm = \"pred\"  # set normalisation rule for the confusion matrices (true/pred/all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500a9855",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# data folder(s) (shall have \"/\" at the end):\n",
    "masterpath = \"/DATA_MASTER_PATH/\"\n",
    "datapath = [\n",
    "    masterpath + \"23_hike_pinunu-background/2305_zoptical-zanalyze_training/\",\n",
    "    #masterpath + \"23_hike_pinunu-background/2306_zoptical-zanalyze_lambda_mass_prod/\",\n",
    "    #masterpath + \"23_hike_pinunu-background/2306_zoptical-zanalyze_2pi_mass_prod/\",\n",
    "]\n",
    "\n",
    "# data selection criteria, set here:\n",
    "datasel = lambda name : (\"2305_zoptical-zanalyze_training\" in name) & (\".csv\" in name) & (\"_V2_\" in name)\n",
    "#def datasel(name):\n",
    "#    sel = (\"2305_zoptical-zanalyze_training\" in name) & (\".csv\" in name) & (\"_V2_\" in name)\n",
    "#    for i in range(100):  # this limits the dataset to a fixed amount of mass-production files\n",
    "#        if i<11:\n",
    "#            sel = sel | (\n",
    "#                (\"2306_zoptical-zanalyze_lambda_mass_prod\" in name) &\\\n",
    "#                (\"040623\" in name) &\\\n",
    "#                (\"_\"+str(i)+\".\" in name)\n",
    "#            )\n",
    "#    return sel\n",
    "\n",
    "filenames = []\n",
    "for s in datapath:\n",
    "    filenames += [s+f.name for f in list(os.scandir(s)) if datasel(s+f.name)]\n",
    "print(\"list of files to open:\")\n",
    "print(*filenames, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e554371",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# open data\n",
    "df = pd.DataFrame()\n",
    "for name in filenames:\n",
    "    print(\"opening file %s\" % name.rsplit('/', 1)[-1])\n",
    "    df0 = pd.read_csv(name)\n",
    "    df0[\"file\"] = name.replace(\".csv\", \"\")\n",
    "    #df = pd.concat([df, df0], ignore_index=True, sort=False)\n",
    "    df = pd.concat([df, df0[  # quick trick to deal with lambda high (useless) statistics\n",
    "        ((df0[\"class\"]==classlabel[\"lambda-pin\"]) & (df0.W>limLambdaWeights[0]) & (df0.W<limLambdaWeights[1])) |\\\n",
    "        (df0[\"class\"]!=classlabel[\"lambda-pin\"])\n",
    "    ]], ignore_index=True, sort=False)\n",
    "    del df0\n",
    "    \n",
    "# duplicate raw weights\n",
    "df[\"W0\"] = df.W\n",
    "    \n",
    "# there are faulty vertex (as reconstructed by the calorimeter) data --> throwing them away if needed\n",
    "if bTossFaultyVertex:\n",
    "    print(\"removing %d broken-vertex events (%f%%)\" % (\n",
    "        df[df.Vertex_xRec_Z.isnull()].shape[0], df[df.Vertex_xRec_Z.isnull()].shape[0]/df.shape[0]*100\n",
    "    ))\n",
    "    df = df[~df.Vertex_xRec_Z.isnull()]\n",
    "    \n",
    "# crude nr. of events\n",
    "print(\"---\")\n",
    "print(\"total events: %d\" % df.shape[0])\n",
    "for i_class in df[\"class\"].unique():\n",
    "    s_class=[s for s in classlabel if classlabel[s]==i_class][0]\n",
    "    print(\"events in class %d (%s): %d\" % (\n",
    "        i_class, s_class, df[df[\"class\"]==i_class].shape[0])\n",
    "    )\n",
    "\n",
    "# sum of event weights\n",
    "sumWOrig = {}\n",
    "for i_class in df[\"class\"].unique():\n",
    "    s_class=[s for s in classlabel if classlabel[s]==i_class][0]\n",
    "    print(\n",
    "        \"sum of weights in class %s: %f\" % (\n",
    "            s_class, sum(df[df[\"class\"]==classlabel[s_class]].W0)\n",
    "        )\n",
    "    )\n",
    "    sumWOrig.update({i_class: sum(df[df[\"class\"]==classlabel[s_class]].W0)})\n",
    "    \n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b25304e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# cut on lambda weights\n",
    "\n",
    "if limLambdaWeights[0]>0:\n",
    "    s_class = \"lambda-pin\"\n",
    "    i_class = classlabel[s_class]\n",
    "    \n",
    "    df = df[\n",
    "        ((df[\"class\"]==i_class) & (df.W0>limLambdaWeights[0]) & (df.W0<limLambdaWeights[1])) |\\\n",
    "        (df[\"class\"]!=i_class)\n",
    "    ]\n",
    "    \n",
    "    print(\"after selecting only (lambda) weights > %.2E & < %.2E:\" % (limLambdaWeights[0], limLambdaWeights[1]))\n",
    "    print(\"events in class %d (%s): %d\" % (\n",
    "        i_class, s_class, df[df[\"class\"]==i_class].shape[0])\n",
    "    )\n",
    "    print(\n",
    "        \"sum of weights in class %s: %f\" % (\n",
    "            s_class, sum(df[df[\"class\"]==i_class].W0)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"keeping the whole lambda distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6063c195",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# only keep events in the blind analysis signal box, if needed\n",
    "\n",
    "fv = (fv_edges[0]-shift_fv, fv_edges[1]-shift_fv)\n",
    "\n",
    "if bOnlyInFV | bOnlyInPtCut:\n",
    "    x_box = df.Vertex_xRec_Z\n",
    "    y_box = np.sqrt(\n",
    "        (df.Vertex_pRec0_X + df.Vertex_pRec1_X)**2 + (df.Vertex_pRec0_Y + df.Vertex_pRec1_Y)**2\n",
    "    )\n",
    "    \n",
    "    bool_fv = ((x_box > fv[0]) & (x_box < fv[1])) \\\n",
    "        if bOnlyInFV else (x_box < 1e10)\n",
    "    bool_pt = y_box > (pt_cut if bOnlyInPtCut else -1)\n",
    "    \n",
    "    df = df[bool_fv & bool_pt]\n",
    "    \n",
    "    print(\"after limiting to the blind analysis box:\")\n",
    "    \n",
    "    # crude nr. of events\n",
    "    print(\"total events: %d\" % df.shape[0])\n",
    "    for i_class in df[\"class\"].unique():\n",
    "        s_class=[s for s in classlabel if classlabel[s]==i_class][0]\n",
    "        print(\"events in class %d (%s): %d\" % (\n",
    "            i_class, s_class, df[df[\"class\"]==i_class].shape[0])\n",
    "        )\n",
    "            \n",
    "    # sum of event weights\n",
    "    sumWOrig = {}  # if limiting to the FV and/or the cut in pt, reset sumWOrig\n",
    "    for i_class in df[\"class\"].unique():\n",
    "        s_class=[s for s in classlabel if classlabel[s]==i_class][0]\n",
    "        print(\n",
    "            \"sum of weights in class %s: %f\" % (\n",
    "                s_class, sum(df[df[\"class\"]==classlabel[s_class]].W0)\n",
    "            )\n",
    "        )\n",
    "        sumWOrig.update({i_class: sum(df[df[\"class\"]==classlabel[s_class]].W0)})\n",
    "        \n",
    "else:\n",
    "    print(\"keeping the whole final-variable phase space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbef50e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# descale to a fixed dataset size (per class)\n",
    "\n",
    "if not (nEvsPop is None):  # deactivate setting nEvsPop=None above\n",
    "    print(\"after descaling (w/ %d events per class - *%d in case of lambda):\" % (nEvsPop, nMultLambda))\n",
    "\n",
    "    classdf = []\n",
    "    for i, i_class in enumerate(df[\"class\"].unique()):\n",
    "        if bBinaryClass:\n",
    "            classdf.append(\n",
    "                df[df[\"class\"]==i_class].sample(frac=1).head(\n",
    "                    nEvsPop \\\n",
    "                    if (i_class!=classlabel[\"lambda-pin\"]) else \\\n",
    "                    nMultLambda*nEvsPop\n",
    "                ))\n",
    "        else:\n",
    "            classdf.append(\n",
    "                df[df[\"class\"]==i_class].sample(frac=1).head(\n",
    "                    nEvsPop \\\n",
    "                    if (i_class==classlabel[whichsig]) else \\\n",
    "                    int(nEvsPop / (len(df[\"class\"].unique())-1))\n",
    "                ))\n",
    "        print(\"events in class %d: %d\" % (i_class, classdf[i].shape[0]))\n",
    "\n",
    "    df = pd.concat(classdf)\n",
    "    print(\"total events: %d\" % df.shape[0])\n",
    "    \n",
    "else:\n",
    "    print(\"no descaling is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1279036",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# fix the event weights - part 1/2 (must be done before manipulating classes)\n",
    "\n",
    "for i_class in df[\"class\"].unique():\n",
    "    s_class = [s for s in classlabel.keys() if classlabel[s]==i_class][0]\n",
    "\n",
    "    df.loc[df[\"class\"]==i_class, \"W2\"] = df.W0 * (\n",
    "        (classbr[s_class]*classflux[s_class]) / \\\n",
    "        (classngen[s_class] * (sum(df[df[\"class\"]==classlabel[s_class]].W0)/sumWOrig[classlabel[s_class]]))\n",
    "        # num. is the nr. of expected decays in the experiment (overall, not only the properly detected ones)\n",
    "        # den. is the nr. of events initially simulated in zOptical (rescaled to match the nEvsPop subsel.)\n",
    "    )\n",
    "    print(\n",
    "        \"class %s, sum of weights (after descaling and rescaling - W2): %f\" % (\n",
    "            s_class, sum(df[df[\"class\"]==classlabel[s_class]].W2)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d84aa8a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# check all available classes\n",
    "\n",
    "for i_class in df[\"class\"].unique():\n",
    "    print(\n",
    "        \"class = %d <--> filename type is e.g. %s\" % (\n",
    "            i_class, str(df[df[\"class\"] == i_class].file.head(1).values[0]).rsplit('/', 1)[-1]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "df[\"class0\"] = df[\"class\"]\n",
    "\n",
    "if bBinaryClass:\n",
    "    df = df[(df[\"class\"]==classlabel[whichsig]) | (df[\"class\"]==classlabel[whichbkg])]\n",
    "    print(\"only keeping %s (signal) and %s (background)\" % (whichsig, whichbkg))\n",
    "else:\n",
    "    df.loc[df[\"class\"]!=classlabel[whichsig], \"class\"] = classlabel[\"background\"]\n",
    "    whichbkg = \"background\"\n",
    "    print(\n",
    "        \"keeping all the available channels (%d) but turning all but signal into tot. background (%d)\" \\\n",
    "        % (len(df[\"class\"].unique()), classlabel[\"background\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be299d10",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# fix the event weights - part 2/2 (must be done after manipulating classes)\n",
    "\n",
    "for i_class in df[\"class\"].unique():\n",
    "    s_class = [s for s in classlabel.keys() if classlabel[s]==i_class][0]\n",
    "    \n",
    "    df.loc[df[\"class\"]==i_class, \"W1\"] = df.W0 / sum(df[df[\"class\"]==i_class].W0)\n",
    "    print(\n",
    "        \"class %s, sum of weights (after descaling and normalising - W1): %f\" % (\n",
    "            s_class, sum(df[df[\"class\"]==classlabel[s_class]].W1)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "df[\"W\"] = (df.W2) if bReweightStat else (df.W1)\n",
    "s = \"eventually using weights \"+(\"normalised to 1\" if (not bReweightStat) else \"rescaled to the exp. stat.\")+\" per class\"\n",
    "s += (\" (W1)\" if (not bReweightStat) else \" (W2)\")\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5056b36e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"list of variables (%d):\" % df.shape[1])\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4ae122",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# create classification datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e328ca",
   "metadata": {},
   "source": [
    "creating\n",
    "\n",
    "- `y`, series with class indexex\n",
    "- `W`, series with weights\n",
    "- `X`, dataframe with only the variables selected for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143f400c",
   "metadata": {},
   "source": [
    "### CLASSES AND WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d82863",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# class index - signal will be 1, background will be 0\n",
    "y0 = label_binarize(df[\"class\"].copy(), classes=[classlabel[whichbkg], classlabel[whichsig]], neg_label=0, pos_label=1)\n",
    "y0 = np.ravel(y0)\n",
    "y = pd.Series(data=y0, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fc550e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# event weights\n",
    "W = df[\"W\"].copy()  # can be physical weights or normalised weights, depending on bReweightStat\n",
    "WPhys = df[\"W2\"].copy()  # is always the physical weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad53c622",
   "metadata": {},
   "source": [
    "### VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bce5e3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# classification variables (only the \"actually measured\" quantities & all those derived from them)\n",
    "\n",
    "# select best vertex quantities\n",
    "df.loc[df.Vertex_nConverted==0, \"Vertex_xBest_Z\"] = df.Vertex_xRec_Z\n",
    "df.loc[df.Vertex_nConverted!=0, \"Vertex_xBest_Z\"] = df.Vertex_xRecPre_Z\n",
    "df.loc[df.Vertex_nConverted==0, \"Vertex_xBest_X\"] = df.Vertex_xRec_X\n",
    "df.loc[df.Vertex_nConverted!=0, \"Vertex_xBest_X\"] = df.Vertex_xRecPre_X\n",
    "df.loc[df.Vertex_nConverted==0, \"Vertex_xBest_Y\"] = df.Vertex_xRec_Y\n",
    "df.loc[df.Vertex_nConverted!=0, \"Vertex_xBest_Y\"] = df.Vertex_xRecPre_Y\n",
    "df.loc[df.Vertex_nConverted==0, \"Vertex_pBest0_Z\"] = df.Vertex_pRec0_Z\n",
    "df.loc[df.Vertex_nConverted!=0, \"Vertex_pBest0_Z\"] = df.Vertex_pRecPre0_Z\n",
    "df.loc[df.Vertex_nConverted==0, \"Vertex_pBest0_X\"] = df.Vertex_pRec0_X\n",
    "df.loc[df.Vertex_nConverted!=0, \"Vertex_pBest0_X\"] = df.Vertex_pRecPre0_X\n",
    "df.loc[df.Vertex_nConverted==0, \"Vertex_pBest0_Y\"] = df.Vertex_pRec0_Y\n",
    "df.loc[df.Vertex_nConverted!=0, \"Vertex_pBest0_Y\"] = df.Vertex_pRecPre0_Y\n",
    "df.loc[df.Vertex_nConverted==0, \"Vertex_pBest1_Z\"] = df.Vertex_pRec1_Z\n",
    "df.loc[df.Vertex_nConverted!=0, \"Vertex_pBest1_Z\"] = df.Vertex_pRecPre1_Z\n",
    "df.loc[df.Vertex_nConverted==0, \"Vertex_pBest1_X\"] = df.Vertex_pRec1_X\n",
    "df.loc[df.Vertex_nConverted!=0, \"Vertex_pBest1_X\"] = df.Vertex_pRecPre1_X\n",
    "df.loc[df.Vertex_nConverted==0, \"Vertex_pBest1_Y\"] = df.Vertex_pRec1_Y\n",
    "df.loc[df.Vertex_nConverted!=0, \"Vertex_pBest1_Y\"] = df.Vertex_pRecPre1_Y\n",
    "\n",
    "# compute all transverse quantities from cartesian components\n",
    "df[\"Vertex_xRec_T\"] = np.sqrt( df.Vertex_xRec_X**2 + df.Vertex_xRec_Y**2 )\n",
    "df[\"Vertex_xRecPre_T\"] = np.sqrt( df.Vertex_xRecPre_X**2 + df.Vertex_xRecPre_Y**2 )\n",
    "df[\"Cluster0_xRec_T\"] = np.sqrt( df.Cluster0_xRec_X**2 + df.Cluster0_xRec_Y**2 )\n",
    "df[\"Cluster1_xRec_T\"] = np.sqrt( df.Cluster1_xRec_X**2 + df.Cluster1_xRec_Y**2 )\n",
    "df[\"Vertex_pRec0_T\"] = np.sqrt( df.Vertex_pRec0_X**2 + df.Vertex_pRec0_Y**2 )\n",
    "df[\"Vertex_pRec1_T\"] = np.sqrt( df.Vertex_pRec1_X**2 + df.Vertex_pRec1_Y**2 )\n",
    "df[\"Cluster0_xPre1_T\"] = np.sqrt( df.Cluster0_xPre1_X**2 + df.Cluster0_xPre1_Y**2 )\n",
    "df[\"Cluster0_xPre2_T\"] = np.sqrt( df.Cluster0_xPre2_X**2 + df.Cluster0_xPre2_Y**2 )\n",
    "df[\"Cluster1_xPre1_T\"] = np.sqrt( df.Cluster1_xPre1_X**2 + df.Cluster1_xPre1_Y**2 )\n",
    "df[\"Cluster1_xPre2_T\"] = np.sqrt( df.Cluster1_xPre2_X**2 + df.Cluster1_xPre2_Y**2 )\n",
    "df[\"Vertex_pRecPre0_T\"] = np.sqrt( df.Vertex_pRecPre0_X**2 + df.Vertex_pRecPre0_Y**2 )\n",
    "df[\"Vertex_pRecPre1_T\"] = np.sqrt( df.Vertex_pRecPre1_X**2 + df.Vertex_pRecPre1_Y**2 )\n",
    "\n",
    "df[\"Vertex_xBest_T\"] = np.sqrt( df.Vertex_xBest_X**2 + df.Vertex_xBest_Y**2 )\n",
    "df[\"Vertex_pBest0_T\"] = np.sqrt( df.Vertex_pBest0_X**2 + df.Vertex_pBest0_Y**2 )\n",
    "df[\"Vertex_pBest1_T\"] = np.sqrt( df.Vertex_pBest1_X**2 + df.Vertex_pBest1_Y**2 )\n",
    "\n",
    "# compute new 2-cluster position and momentum-related variables\n",
    "df[\"Clusters_xRec_TMin\"] = df[[\"Cluster0_xRec_T\", \"Cluster1_xRec_T\"]].min(axis=1)\n",
    "df[\"Clusters_xPre1_TMin\"] = df[[\"Cluster0_xPre1_T\", \"Cluster1_xPre1_T\"]].min(axis=1)\n",
    "df[\"Clusters_xPre2_TMin\"] = df[[\"Cluster0_xPre2_T\", \"Cluster1_xPre2_T\"]].min(axis=1)\n",
    "df[\"Vertex_pRec_TMin\"] = df[[\"Vertex_pRec0_T\", \"Vertex_pRec1_T\"]].min(axis=1)\n",
    "df[\"Vertex_pRecPre_TMin\"] = df[[\"Vertex_pRecPre0_T\", \"Vertex_pRecPre1_T\"]].min(axis=1)\n",
    "df[\"Clusters_xRec_TMax\"] = df[[\"Cluster0_xRec_T\", \"Cluster1_xRec_T\"]].max(axis=1)\n",
    "df[\"Clusters_xPre1_TMax\"] = df[[\"Cluster0_xPre1_T\", \"Cluster1_xPre1_T\"]].max(axis=1)\n",
    "df[\"Clusters_xPre2_TMax\"] = df[[\"Cluster0_xPre2_T\", \"Cluster1_xPre2_T\"]].max(axis=1)\n",
    "df[\"Vertex_pRec_TMax\"] = df[[\"Vertex_pRec0_T\", \"Vertex_pRec1_T\"]].max(axis=1)\n",
    "df[\"Vertex_pRecPre_TMax\"] = df[[\"Vertex_pRecPre0_T\", \"Vertex_pRecPre1_T\"]].max(axis=1)\n",
    "df[\"Clusters_xRec_TSum\"] = df.Clusters_xRec_TMax + df.Clusters_xRec_TMin\n",
    "df[\"Clusters_xPre1_TSum\"] = df.Clusters_xPre1_TMax + df.Clusters_xPre1_TMin\n",
    "df[\"Clusters_xPre2_TSum\"] = df.Clusters_xPre2_TMax + df.Clusters_xPre2_TMin\n",
    "df[\"Vertex_pRec_TSum\"] = df.Vertex_pRec_TMax + df.Vertex_pRec_TMin\n",
    "df[\"Vertex_pRecPre_TSum\"] = df.Vertex_pRecPre_TMax + df.Vertex_pRecPre_TMin\n",
    "df[\"Clusters_xRec_TDif\"] = df.Clusters_xRec_TMax - df.Clusters_xRec_TMin\n",
    "df[\"Clusters_xPre1_TDif\"] = df.Clusters_xPre1_TMax - df.Clusters_xPre1_TMin\n",
    "df[\"Clusters_xPre2_TDif\"] = df.Clusters_xPre2_TMax - df.Clusters_xPre2_TMin\n",
    "df[\"Vertex_pRec_TDif\"] = df.Vertex_pRec_TMax - df.Vertex_pRec_TMin\n",
    "df[\"Vertex_pRecPre_TDif\"] = df.Vertex_pRecPre_TMax - df.Vertex_pRecPre_TMin\n",
    "df[\"Clusters_xRec_TAsym\"] = df.Clusters_xRec_TDif / df.Clusters_xRec_TSum\n",
    "df[\"Clusters_xPre1_TAsym\"] = df.Clusters_xPre1_TDif / df.Clusters_xPre1_TSum\n",
    "df[\"Clusters_xPre2_TAsym\"] = df.Clusters_xPre2_TDif / df.Clusters_xPre2_TSum\n",
    "df[\"Vertex_pRec_TAsym\"] = df.Vertex_pRec_TDif / df.Vertex_pRec_TSum\n",
    "df[\"Vertex_pRecPre_TAsym\"] = df.Vertex_pRecPre_TDif / df.Vertex_pRecPre_TSum\n",
    "\n",
    "df[\"Vertex_pBest_TMin\"] = df[[\"Vertex_pBest0_T\", \"Vertex_pBest1_T\"]].min(axis=1)\n",
    "df[\"Vertex_pBest_TMax\"] = df[[\"Vertex_pBest0_T\", \"Vertex_pBest1_T\"]].max(axis=1)\n",
    "df[\"Vertex_pBest_TSum\"] = df.Vertex_pBest_TMax + df.Vertex_pBest_TMin\n",
    "df[\"Vertex_pBest_TDif\"] = df.Vertex_pBest_TMax - df.Vertex_pBest_TMin\n",
    "df[\"Vertex_pBest_TAsym\"] = df.Vertex_pBest_TDif / df.Vertex_pBest_TSum\n",
    "\n",
    "df[\"Clusters_xDist\"] = np.sqrt(  # distance between clusters\n",
    "    (df.Cluster1_xRec_X-df.Cluster0_xRec_X)**2 + (df.Cluster1_xRec_Y-df.Cluster0_xRec_Y)**2\n",
    ")\n",
    "\n",
    "# compute new 2-cluster energy-related variables\n",
    "df[\"Clusters_EMin\"] = df[[\"Cluster0_ERec\", \"Cluster1_ERec\"]].min(axis=1)\n",
    "df[\"Clusters_EMax\"] = df[[\"Cluster0_ERec\", \"Cluster1_ERec\"]].max(axis=1)\n",
    "df[\"Clusters_ESum\"] = df.Clusters_EMax + df.Clusters_EMin\n",
    "df[\"Clusters_EDif\"] = df.Clusters_EMax - df.Clusters_EMin\n",
    "df[\"Clusters_EAsym\"] = df.Clusters_EDif / df.Clusters_ESum\n",
    "\n",
    "df.loc[df.Cluster0_ERec==df.Clusters_EMin, \"Clusters_xRec_TEMin\"] = df.Cluster0_xRec_T\n",
    "df.loc[df.Cluster0_ERec==df.Clusters_EMin, \"Clusters_xRec_TEMax\"] = df.Cluster1_xRec_T\n",
    "df.loc[df.Cluster1_ERec==df.Clusters_EMin, \"Clusters_xRec_TEMin\"] = df.Cluster1_xRec_T\n",
    "df.loc[df.Cluster1_ERec==df.Clusters_EMin, \"Clusters_xRec_TEMax\"] = df.Cluster0_xRec_T\n",
    "\n",
    "df[\"Clusters_ECOG\"] = np.sqrt(  # cog. between clusters (weighted with energies)\n",
    "    (\n",
    "        (df.Cluster0_ERec*df.Cluster0_xRec_X + df.Cluster1_ERec*df.Cluster1_xRec_X)**2 +\\\n",
    "        (df.Cluster0_ERec*df.Cluster0_xRec_Y + df.Cluster1_ERec*df.Cluster1_xRec_Y)**2\n",
    "    ) / df.Clusters_ESum**2\n",
    ")\n",
    "\n",
    "# compute pion kinematics from two photons (pion energy is simply Clusters_ESum)\n",
    "df[\"Vertex_pRecPi_Z\"] = df.Vertex_pRec0_Z + df.Vertex_pRec1_Z\n",
    "df[\"Vertex_pRecPi_X\"] = df.Vertex_pRec0_X + df.Vertex_pRec1_X\n",
    "df[\"Vertex_pRecPi_Y\"] = df.Vertex_pRec0_Y + df.Vertex_pRec1_Y\n",
    "df[\"Vertex_pRecPrePi_Z\"] = df.Vertex_pRecPre0_Z + df.Vertex_pRecPre1_Z\n",
    "df[\"Vertex_pRecPrePi_X\"] = df.Vertex_pRecPre0_X + df.Vertex_pRecPre1_X\n",
    "df[\"Vertex_pRecPrePi_Y\"] = df.Vertex_pRecPre0_Y + df.Vertex_pRecPre1_Y\n",
    "\n",
    "df[\"Vertex_pRecPi_T\"] = np.sqrt( df.Vertex_pRecPi_X**2 + df.Vertex_pRecPi_Y**2 )\n",
    "df[\"Vertex_pRecPrePi_T\"] = np.sqrt( df.Vertex_pRecPrePi_X**2 + df.Vertex_pRecPrePi_Y**2 )\n",
    "\n",
    "df[\"Vertex_pRecPi_sTh\"] = df.Vertex_pRecPi_T / np.sqrt(  # pion propagation angle wrt. beam\n",
    "    df.Vertex_pRecPi_T**2 + df.Vertex_pRecPi_Z**2\n",
    ")\n",
    "df[\"Vertex_pRecPrePi_sTh\"] = df.Vertex_pRecPrePi_T / np.sqrt(  # pion propagation angle wrt. beam\n",
    "    df.Vertex_pRecPrePi_T**2 + df.Vertex_pRecPrePi_Z**2\n",
    ")\n",
    "\n",
    "df[\"Vertex_pBestPi_Z\"] = df.Vertex_pBest0_Z + df.Vertex_pBest1_Z\n",
    "df[\"Vertex_pBestPi_X\"] = df.Vertex_pBest0_X + df.Vertex_pBest1_X\n",
    "df[\"Vertex_pBestPi_Y\"] = df.Vertex_pBest0_Y + df.Vertex_pBest1_Y\n",
    "\n",
    "df[\"Vertex_pBestPi_T\"] = np.sqrt( df.Vertex_pBestPi_X**2 + df.Vertex_pBestPi_Y**2 )\n",
    "\n",
    "df[\"Vertex_pBestPi_sTh\"] = df.Vertex_pBestPi_T / np.sqrt(  # pion propagation angle wrt. beam\n",
    "    df.Vertex_pBestPi_T**2 + df.Vertex_pBestPi_Z**2\n",
    ")\n",
    "\n",
    "# new ideas\n",
    "df.loc[df.Vertex_nConverted==0, \"Vertex_x_ZDiffPreRec\"] = 9999\n",
    "df.loc[df.Vertex_nConverted!=0, \"Vertex_x_ZDiffPreRec\"] = df.Vertex_xRec_Z - df.Vertex_xRecPre_Z \n",
    "# ^^^ Matt: diff. between vertex position from MEC only and from MEC+PSD, longitudinal\n",
    "\n",
    "df.loc[df.Vertex_nConverted==0, \"Vertex_x_TDiffPreRec\"] = 9999\n",
    "df.loc[df.Vertex_nConverted!=0, \"Vertex_x_TDiffPreRec\"] = df.Vertex_xRec_T - df.Vertex_xRecPre_T  \n",
    "# ^^^ Matt: diff. between vertex position from MEC only and from MEC+PSD, transverse\n",
    "\n",
    "df.loc[df.Vertex_nConverted==0, \"Vertex_x_DistDiffPreRec\"] = 9999\n",
    "df.loc[df.Vertex_nConverted!=0, \"Vertex_x_DistDiffPreRec\"] = np.sqrt(  \n",
    "    (df.Vertex_xRec_X - df.Vertex_xRecPre_X)**2 +\\\n",
    "    (df.Vertex_xRec_Y - df.Vertex_xRecPre_Y)**2 +\\\n",
    "    (df.Vertex_xRec_Z - df.Vertex_xRecPre_Z)**2\n",
    ")  # Matt: diff. between vertex position from MEC only and from MEC+PSD, total\n",
    "\n",
    "df[\"Clusters_ERatio\"] = df.Clusters_EMin / df.Clusters_EMax  # from KOTO - lowest-to-highest energy ratio\n",
    "\n",
    "df[\"Vertex_pRec0_sTh\"] = df.Vertex_pRec0_T / np.sqrt(\n",
    "    df.Vertex_pRec0_T**2 + df.Vertex_pRec0_Z**2\n",
    ")\n",
    "df[\"Vertex_pRec1_sTh\"] = df.Vertex_pRec1_T / np.sqrt(\n",
    "    df.Vertex_pRec1_T**2 + df.Vertex_pRec1_Z**2\n",
    ")\n",
    "df[\"Vertex_pRecPre0_sTh\"] = df.Vertex_pRecPre0_T / np.sqrt(\n",
    "    df.Vertex_pRecPre0_T**2 + df.Vertex_pRecPre0_Z**2\n",
    ")\n",
    "df[\"Vertex_pRecPre1_sTh\"] = df.Vertex_pRecPre1_T / np.sqrt(\n",
    "    df.Vertex_pRecPre1_T**2 + df.Vertex_pRecPre1_Z**2\n",
    ")\n",
    "df[\"Vertex_Rec0_EsTh\"] = df.Cluster0_ERec*df.Vertex_pRec0_sTh \n",
    "df[\"Vertex_Rec1_EsTh\"] = df.Cluster1_ERec*df.Vertex_pRec1_sTh\n",
    "df[\"Vertex_RecPre0_EsTh\"] = df.Cluster0_ERec*df.Vertex_pRecPre0_sTh\n",
    "df[\"Vertex_RecPre1_EsTh\"] = df.Cluster1_ERec*df.Vertex_pRecPre1_sTh\n",
    "# ^^^ KOTO - single-photon angle*energy \n",
    "\n",
    "df.loc[df.Cluster0_ERec==df.Clusters_EMin, \"Vertex_Rec_EMinsTh\"] = df.Vertex_Rec0_EsTh\n",
    "df.loc[df.Cluster0_ERec==df.Clusters_EMin, \"Vertex_Rec_EMaxsTh\"] = df.Vertex_Rec1_EsTh\n",
    "df.loc[df.Cluster1_ERec==df.Clusters_EMin, \"Vertex_Rec_EMinsTh\"] = df.Vertex_Rec1_EsTh\n",
    "df.loc[df.Cluster1_ERec==df.Clusters_EMin, \"Vertex_Rec_EMaxsTh\"] = df.Vertex_Rec0_EsTh\n",
    "df.loc[df.Cluster0_ERec==df.Clusters_EMin, \"Vertex_RecPre_EMinsTh\"] = df.Vertex_RecPre0_EsTh\n",
    "df.loc[df.Cluster0_ERec==df.Clusters_EMin, \"Vertex_RecPre_EMaxsTh\"] = df.Vertex_RecPre1_EsTh\n",
    "df.loc[df.Cluster1_ERec==df.Clusters_EMin, \"Vertex_RecPre_EMinsTh\"] = df.Vertex_RecPre1_EsTh\n",
    "df.loc[df.Cluster1_ERec==df.Clusters_EMin, \"Vertex_RecPre_EMaxsTh\"] = df.Vertex_RecPre0_EsTh\n",
    "# ^^^ KOTO - single-photon angle*energy \n",
    "\n",
    "df[\"Vertex_pBest0_sTh\"] = df.Vertex_pBest0_T / np.sqrt(\n",
    "    df.Vertex_pBest0_T**2 + df.Vertex_pBest0_Z**2\n",
    ")\n",
    "df[\"Vertex_pBest1_sTh\"] = df.Vertex_pBest1_T / np.sqrt(\n",
    "    df.Vertex_pBest1_T**2 + df.Vertex_pBest1_Z**2\n",
    ")\n",
    "df[\"Vertex_Best0_EsTh\"] = df.Cluster0_ERec*df.Vertex_pBest0_sTh\n",
    "df[\"Vertex_Best1_EsTh\"] = df.Cluster1_ERec*df.Vertex_pBest1_sTh\n",
    "# ^^^ KOTO - single-photon angle*energy \n",
    "\n",
    "df.loc[df.Cluster0_ERec==df.Clusters_EMin, \"Vertex_Best_EMinsTh\"] = df.Vertex_Best0_EsTh\n",
    "df.loc[df.Cluster0_ERec==df.Clusters_EMin, \"Vertex_Best_EMaxsTh\"] = df.Vertex_Best1_EsTh\n",
    "df.loc[df.Cluster1_ERec==df.Clusters_EMin, \"Vertex_Best_EMinsTh\"] = df.Vertex_Best1_EsTh\n",
    "df.loc[df.Cluster1_ERec==df.Clusters_EMin, \"Vertex_Best_EMaxsTh\"] = df.Vertex_Best0_EsTh\n",
    "# ^^^ KOTO - single-photon angle*energy \n",
    "\n",
    "df[\"Vertex_pRec0_sPhi\"] = np.arcsin(df.Vertex_pRec0_Y / df.Vertex_pRec0_T)\n",
    "df[\"Vertex_pRec1_sPhi\"] = np.arcsin(df.Vertex_pRec1_Y / df.Vertex_pRec1_T)\n",
    "df[\"Vertex_pRecPre0_sPhi\"] = np.arcsin(df.Vertex_pRecPre0_Y / df.Vertex_pRecPre0_T)\n",
    "df[\"Vertex_pRecPre1_sPhi\"] = np.arcsin(df.Vertex_pRecPre1_Y / df.Vertex_pRecPre1_T)\n",
    "df[\"Vertex_pRec_dsPhi\"] = np.abs(df.Vertex_pRec1_sPhi - df.Vertex_pRec0_sPhi)\n",
    "df[\"Vertex_pRecPre_dsPhi\"] = np.abs(df.Vertex_pRecPre1_sPhi - df.Vertex_pRecPre0_sPhi)\n",
    "# ^^^ KOTO - transv. angle between photons\n",
    "    \n",
    "df[\"Vertex_pBest0_sPhi\"] = np.arcsin(df.Vertex_pBest0_Y / df.Vertex_pBest0_T)\n",
    "df[\"Vertex_pBest1_sPhi\"] = np.arcsin(df.Vertex_pBest1_Y / df.Vertex_pBest1_T)\n",
    "df[\"Vertex_pBest_dsPhi\"] = np.abs(df.Vertex_pBest1_sPhi - df.Vertex_pBest0_sPhi)\n",
    "# ^^^ KOTO - transv. angle between photons\n",
    "\n",
    "print(\"(preliminary) nr. of classification variables after calculations: %d\" % df.shape[1])\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5809ad24",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# final variable selection\n",
    "\n",
    "X = df[[\n",
    "    \n",
    "    # variables for final analysis\n",
    "    #\"Vertex_xBest_Z\",\n",
    "    #\"Vertex_pBestPi_T\",\n",
    "\n",
    "    # vertex data (computed either with MEC only or with MEC & PSD)\n",
    "    \"Vertex_xBest_T\",\n",
    "\n",
    "    # cluster data - positions on MEC\n",
    "    #\"Clusters_xRec_TMin\",  # highly correlated with Clusters_xDist\n",
    "    #\"Clusters_xRec_TMax\",  # highly correlated with Clusters_xDist\n",
    "    \"Clusters_xRec_TAsym\",\n",
    "    \"Clusters_xDist\",\n",
    "    \"Clusters_ECOG\",\n",
    "\n",
    "    # cluster data - energy\n",
    "    \"Clusters_EMin\",\n",
    "    #\"Clusters_EMax\",  # highly correlated with Clusters_ESum\n",
    "    \"Clusters_ESum\",\n",
    "    #\"Clusters_EAsym\",  # highly correlated with Clusters_ERatio\n",
    "    \n",
    "    # new ideas\n",
    "    \"Vertex_x_ZDiffPreRec\",\n",
    "    #\"Vertex_x_TDiffPreRec\",\n",
    "    #\"Vertex_x_DistDiffPreRec\",\n",
    "    \"Clusters_ERatio\",\n",
    "    \"Vertex_Best_EMinsTh\",\n",
    "    \"Vertex_Best_EMaxsTh\",\n",
    "    \"Vertex_pBest_dsPhi\",\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab78518",
   "metadata": {},
   "source": [
    "### CHECK QUALITY OF SELECTED VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b86823",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# violin plot, dataset creation\n",
    "\n",
    "bPlotViolin = False  # create and plot it?\n",
    "\n",
    "if bPlotViolin:\n",
    "    \n",
    "    # copy of X with all variables normalised to 1\n",
    "    X_norm = X.copy()\n",
    "    for s in X.columns:\n",
    "        X_norm[s] = (X_norm[s] - X_norm[s].mean()) / X_norm[s].std()\n",
    "    \n",
    "    # dedicated dataframe for violin plot\n",
    "    X_viol = pd.melt(pd.concat([X_norm, y], axis=1), \"class\", var_name=\"feature\")\n",
    "    for s in classlabel:\n",
    "        X_viol.loc[X_viol[\"class\"]==classlabel[s], \"class\"] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab299c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# violin plot, plotting (if bPlotViolin is True and class choice is compatible)\n",
    "\n",
    "if bPlotViolin:\n",
    "    if (bBinaryClass | (not (classlabel[\"lambda-pin\"] in df[\"class\"].unique()))):\n",
    "        bPlotViolin_save = True  # save plot?\n",
    "        scale = \"width\"  # scale heights to the bin entries (count) or to the distr. maxima (width)\n",
    "\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        plt.ylim((-3, 6))\n",
    "        plt.yscale(\"linear\")\n",
    "\n",
    "        viol = sns.violinplot(\n",
    "            x=\"feature\", y=\"value\", hue=\"class\",\n",
    "            split=True, data=X_viol, inner=None, bw=0.05, gridsize=1000, scale=scale\n",
    "        )\n",
    "        viol.set_xticklabels(rotation=90, labels=list(X_norm)) ;\n",
    "\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        if bPlotViolin_save:\n",
    "            plt.savefig(\"./output_misc/violinplot.png\")\n",
    "            \n",
    "    else:\n",
    "        print(\"cannot make a class-comparison violin plot with this class conf.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e23eb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "\n",
    "bCorrMatrix = True  # plot it?\n",
    "\n",
    "if bCorrMatrix:\n",
    "    \n",
    "    bCorrMatrix_save = True  # save plot?\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(X.corr(), linewidths=.1, annot=True, annot_kws={\"size\": 10}, cmap=\"PiYG\", center=0)\n",
    "    plt.yticks(rotation=0) ;\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if bCorrMatrix_save:\n",
    "        plt.savefig(\"./output_misc/correlationmatrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bed92f",
   "metadata": {},
   "source": [
    "### MORE CHECKS, MOSTLY FOR LAMBDA BACKGROUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818309a5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# min, mean, median, max for each variable\n",
    "\n",
    "if False:\n",
    "    for var in X.columns:\n",
    "        print(f\"{var:>23}{X[var].min():^28}{X[var].mean():^23}{X[var].median():^23}{X[var].max():^23}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba2406f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# check all variables with standard histograms -- as a function of the longitudinal position\n",
    "\n",
    "perc_fv = 0.85\n",
    "\n",
    "if False:\n",
    "    for var in X.columns:\n",
    "        fig, ax = plt.subplots(ncols=2, nrows=3, figsize=(12, 10), sharex=True)\n",
    "        ax[0, 0].grid(True)\n",
    "        ax[0, 1].grid(True)\n",
    "        ax[1, 0].grid(True)\n",
    "        ax[1, 1].grid(True)\n",
    "        ax[2, 0].grid(True)\n",
    "        ax[2, 1].grid(True)\n",
    "        ax[0, 0].set_title(\"all events\")\n",
    "        ax[0, 1].set_title(\"no conversions in PSD\")\n",
    "        ax[1, 0].set_title(\"1 conversion in PSD\")\n",
    "        ax[1, 1].set_title(\"2 conversions in PSD\")\n",
    "        ax[2, 0].set_title(\"nonzero, upstream of %d%% of the FV\" % (perc_fv*100))\n",
    "        ax[2, 1].set_title(\"nonzero, downsteam of %d%% of the FV\" % (100-perc_fv*100))\n",
    "        \n",
    "        bins_sp={  # manual tweak to the binning below (l. edge, r. edge, bins for non-lambda, bins for lambda)\n",
    "            \"Vertex_xBest_T\" : (0, 0.3, 120, 60),\n",
    "            \"Clusters_EMin\" : (0, 45, 120, 60),\n",
    "            \"Clusters_ESum\" : (0, 100, 120, 60),\n",
    "            \"Vertex_x_DistDiffPreRec\" : (0, 300, 120, 60),\n",
    "            \"Vertex_x_ZDiffPreRec\" : (-100, 300, 120, 60),\n",
    "            \"Vertex_x_TDiffPreRec\" : (-0.2, 0.1, 120, 60),\n",
    "            \"Vertex_Best_EMinsTh\" : (0, 0.5, 120, 60),\n",
    "            \"Vertex_Best_EMaxsTh\" : (0, 1, 120, 60),\n",
    "        }\n",
    "        bins = (\n",
    "            np.linspace(\n",
    "                min(df[df[\"class\"]!=classlabel[\"lambda-pin\"]][var]) if (not (var in bins_sp)) else bins_sp[var][0],\n",
    "                max(df[df[\"class\"]!=classlabel[\"lambda-pin\"]][var]) if (not (var in bins_sp)) else bins_sp[var][1],\n",
    "                120 if (not (var in bins_sp)) else bins_sp[var][2]  # binning for all the classes but lambda\n",
    "            ),\n",
    "            np.linspace(\n",
    "                min(df[df[\"class\"]!=classlabel[\"lambda-pin\"]][var]) if (not (var in bins_sp)) else bins_sp[var][0],\n",
    "                max(df[df[\"class\"]!=classlabel[\"lambda-pin\"]][var]) if (not (var in bins_sp)) else bins_sp[var][1],\n",
    "                60 if (not (var in bins_sp)) else bins_sp[var][3]  # binning for lambda\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        for i_class in df[\"class\"].unique():\n",
    "            b = (df[\"class\"]==i_class)\n",
    "\n",
    "            ax[0, 0].hist(\n",
    "                df[b][var], \n",
    "                bins=bins[0] if i_class!=classlabel[\"lambda-pin\"] else bins[1], histtype=\"step\", density=True,\n",
    "                label=[s for s in classlabel if classlabel[s]==i_class][0], \n",
    "                weights=df[b].W\n",
    "            )\n",
    "\n",
    "            ax[0, 1].hist(\n",
    "                df[b & (df.Vertex_nConverted==0)][var], \n",
    "                bins=bins[0] if i_class!=classlabel[\"lambda-pin\"] else bins[1], histtype=\"step\", density=True,\n",
    "                label=[s for s in classlabel if classlabel[s]==i_class][0], \n",
    "                weights=df[b & (df.Vertex_nConverted==0)].W\n",
    "            )\n",
    "\n",
    "            ax[1, 0].hist(\n",
    "                df[b & (df.Vertex_nConverted==1)][var], \n",
    "                bins=bins[0] if i_class!=classlabel[\"lambda-pin\"] else bins[1], histtype=\"step\", density=True,\n",
    "                label=[s for s in classlabel if classlabel[s]==i_class][0], \n",
    "                weights=df[b & (df.Vertex_nConverted==1)].W\n",
    "            )\n",
    "\n",
    "            ax[1, 1].hist(\n",
    "                df[b & (df.Vertex_nConverted==2)][var], \n",
    "                bins=bins[0] if i_class!=classlabel[\"lambda-pin\"] else bins[1], histtype=\"step\", density=True,\n",
    "                label=[s for s in classlabel if classlabel[s]==i_class][0], \n",
    "                weights=df[b & (df.Vertex_nConverted==2)].W\n",
    "            )\n",
    "\n",
    "            ax[2, 0].hist(\n",
    "                df[b & (df[var]!=0) & (df.Vertex_xRec_Z<(fv[0]+perc_fv*(fv[1]-fv[0])))][var], \n",
    "                bins=bins[0] if i_class!=classlabel[\"lambda-pin\"] else bins[1], histtype=\"step\", density=True,\n",
    "                label=[s for s in classlabel if classlabel[s]==i_class][0], \n",
    "                weights=df[b & (df[var]!=0) & (df.Vertex_xRec_Z<(fv[0]+perc_fv*(fv[1]-fv[0])))].W\n",
    "            )\n",
    "\n",
    "            ax[2, 1].hist(\n",
    "                df[b & (df[var]!=0) & (df.Vertex_xRec_Z>(fv[0]+perc_fv*(fv[1]-fv[0])))][var], \n",
    "                bins=bins[0] if i_class!=classlabel[\"lambda-pin\"] else bins[1], histtype=\"step\", density=True,\n",
    "                label=[s for s in classlabel if classlabel[s]==i_class][0], \n",
    "                weights=df[b & (df[var]!=0) & (df.Vertex_xRec_Z>(fv[0]+perc_fv*(fv[1]-fv[0])))].W\n",
    "            )\n",
    "\n",
    "        ax[0, 0].legend()\n",
    "        fig.supxlabel(var)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        if True:\n",
    "            plt.savefig(\"./output_misc/check_vars_%s.png\" % var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c51d9f9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# BDT hyperparameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00227f1a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create training/testing datasets for the new model\n",
    "X_train, X_test, y_train, y_test, W_train, W_test, WPhys_train, WPhys_test =\\\n",
    "    train_test_split(X, y, W, WPhys, test_size=split_test_fraction)\n",
    "print(\"total classification dataset size is %d\" % y.shape[0])\n",
    "print(\"training (testing) size is %d (%d)\" % (y_train.shape[0], y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe4e78",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# custom scoring functions\n",
    "\n",
    "hyperparams_opt = {}  # this needs to be equalised even in case the optimisation isn't run\n",
    "\n",
    "## specific scorer for HIKE, binary problem - outdated\n",
    "#def custom_singlebkg_score(\n",
    "#    y_true, y_pred, *,\n",
    "#    whichsig, whichbkg,\n",
    "#    classngen, classlabel, classbr, classflux,\n",
    "#    sample_weight=None, reweight=False, sample_weight_phys=None,\n",
    "#):  \n",
    "#    \n",
    "#    # number of events initially simulated in zOptical (rescaled according to the nEvsPop subselection)\n",
    "#    nact_S = classngen[whichsig] * (len(y_true[y_true==classlabel[whichsig]]) / sumWOrig[classlabel[whichsig]])\n",
    "#    nact_B = classngen[whichbkg] * (len(y_true[y_true==classlabel[whichbkg]]) / sumWOrig[classlabel[whichbkg]])\n",
    "#    \n",
    "#    # number of expected decays in the experiment (overall, not only the properly detected ones)\n",
    "#    nexp_S = classbr[whichsig] * classflux[whichsig]\n",
    "#    nexp_B = classbr[whichbkg] * classflux[whichbkg]\n",
    "#    \n",
    "#    if sample_weight is None:\n",
    "#    # if no weights are provided at all, simply count all the events classified in each class --> bad\n",
    "#        Ss = np.count_nonzeroes((y_pred==classlabel[whichsig]) & (y_true==classlabel[whichsig])) * (nexp_S/nact_S)\n",
    "#        Bs = np.count_nonzeroes((y_pred==classlabel[whichsig]) & (y_true==classlabel[whichbkg])) * (nexp_B/nact_B)  # TBC\n",
    "#        \n",
    "#    else:\n",
    "#        sample_weight = sample_weight.loc[y_true.index.values].values.reshape(-1)\n",
    "#        \n",
    "#        if not reweight:\n",
    "#        # with normalised weights, get the sum of physical (i.e. rescaled) event weights in each class\n",
    "#            if sample_weight_phys==None:  # if no physical weights are given, use the normalised ones\n",
    "#                sample_weight_phys = sample_weight\n",
    "#            Ss = sum(sample_weight_phys[(y_pred==classlabel[whichsig]) & (y_true==classlabel[whichsig])])\n",
    "#            Bs = sum(sample_weight_phys[(y_pred==classlabel[whichsig]) & (y_true==classlabel[whichbkg])])\n",
    "#            \n",
    "#        else:\n",
    "#        # with physical (i.e. rescaled) weights, simply get the weight sum of events in each class\n",
    "#            Ss = sum(sample_weight[(y_pred==classlabel[whichsig]) & (y_true==classlabel[whichsig])])\n",
    "#            Bs = sum(sample_weight[(y_pred==classlabel[whichsig]) & (y_true==classlabel[whichbkg])])\n",
    "#    \n",
    "#    return Ss / np.sqrt(Bs + Ss)\n",
    "\n",
    "## specific scorer for HIKE, binary problem - outdated\n",
    "#def custom_singlebkg_score(\n",
    "#    y_true, y_pred, *,\n",
    "#    whichsig, whichbkg, classlabel\n",
    "#    sample_weight=None, reweight=False, sample_weight_phys=None,\n",
    "#):  \n",
    "#    \n",
    "#    sample_weight = sample_weight.loc[y_true.index.values].values.reshape(-1)\n",
    "#\n",
    "#    if not reweight:\n",
    "#    # with normalised weights, get the sum of physical (i.e. rescaled) event weights in each class, if available\n",
    "#    # (in the scorer only, normalised weights used everywhere else)\n",
    "#        if sample_weight_phys==None:  # if no physical weights are given, use the normalised ones\n",
    "#            sample_weight_phys = sample_weight\n",
    "#        Ss = sum(sample_weight_phys[(y_pred==classlabel[whichsig]) & (y_true==classlabel[whichsig])])\n",
    "#        Bs = sum(sample_weight_phys[(y_pred==classlabel[whichsig]) & (y_true==classlabel[whichbkg])])\n",
    "#\n",
    "#    else:\n",
    "#    # with physical (i.e. rescaled) weights, simply get the weight sum of events in each class\n",
    "#        Ss = sum(sample_weight[(y_pred==classlabel[whichsig]) & (y_true==classlabel[whichsig])])\n",
    "#        Bs = sum(sample_weight[(y_pred==classlabel[whichsig]) & (y_true==classlabel[whichbkg])])\n",
    "#    \n",
    "#    return Ss / np.sqrt(Bs + Ss)\n",
    "\n",
    "# specific scorer for HIKE, binary problem\n",
    "def custom_singlebkg_hike_score(\n",
    "    y_true, y_pred, *,\n",
    "    sample_weight=None, reweight=False, sample_weight_phys=None,\n",
    "):  \n",
    "    \n",
    "    if not (sample_weight is None):\n",
    "        sample_weight = sample_weight.loc[y_true.index.values].values.reshape(-1)\n",
    "    if not (sample_weight_phys is None):\n",
    "        sample_weight_phys = sample_weight_phys.loc[y_true.index.values].values.reshape(-1)\n",
    "        \n",
    "    # compute optimal cut and use it to separate predicted signal and background\n",
    "    sig_cst = []\n",
    "    p_min = min(y_pred)\n",
    "    p_max = max(y_pred)\n",
    "    p_cuts = np.linspace(p_min, p_max, 200)\n",
    "    for p_cut in p_cuts:\n",
    "        y_pred_temp = (y_pred > p_cut).astype('float')\n",
    "        sig_sel = np.count_nonzero(y_pred_temp*y_true)\n",
    "        bkg_sel = np.count_nonzero(y_pred_temp*(1-y_true))\n",
    "        sig_rej = np.count_nonzero((1-y_pred_temp)*y_true)\n",
    "        bkg_rej = np.count_nonzero((1-y_pred_temp)*(1-y_true))\n",
    "        sig_cst.append(\n",
    "            (sig_sel / (sig_sel+bkg_sel)**0.5) if (sig_sel+bkg_sel)!=0 else 0\n",
    "        )\n",
    "    opt_cut = p_cuts[5:-5][np.array(sig_cst[5:-5])==max(sig_cst)][0]\n",
    "    y_pred_int = (y_pred > opt_cut).astype('float')\n",
    "    print(\"--> hike, obtained optimal cut is %f\" % opt_cut)\n",
    "\n",
    "    if not reweight:\n",
    "    # with normalised weights, get the sum of physical (i.e. rescaled) event weights in each class, if available\n",
    "    # (in the scorer only, normalised weights used everywhere else)\n",
    "        if (sample_weight_phys is None):  # if no physical weights are given, use the normalised ones\n",
    "            sample_weight_phys = sample_weight\n",
    "        Ss = sum(sample_weight_phys[(y_pred_int==1) & (y_true==1)])\n",
    "        Bs = sum(sample_weight_phys[(y_pred_int==1) & (y_true==0)])\n",
    "\n",
    "    else:\n",
    "    # with physical (i.e. rescaled) weights, simply get the weight sum of events in each class\n",
    "        Ss = sum(sample_weight[(y_pred_int==1) & (y_true==1)])\n",
    "        Bs = sum(sample_weight[(y_pred_int==1) & (y_true==0)])\n",
    "    \n",
    "    return Ss / np.sqrt(Bs + Ss)\n",
    "\n",
    "# confusion matrix (with optimised cut) determinant, binary problem\n",
    "def custom_singlebkg_confmatrdet_score(\n",
    "    y_true, y_pred, *,\n",
    "    sample_weight=None,\n",
    "):  \n",
    "    \n",
    "    if not (sample_weight is None):\n",
    "        sample_weight = sample_weight.loc[y_true.index.values].values.reshape(-1)\n",
    "    \n",
    "    # compute optimal cut and use it to separate predicted signal and background\n",
    "    sig_cst = []\n",
    "    p_min = min(y_pred)\n",
    "    p_max = max(y_pred)\n",
    "    p_cuts = np.linspace(p_min, p_max, 200)\n",
    "    for p_cut in p_cuts:\n",
    "        y_pred_temp = (y_pred > p_cut).astype('float')\n",
    "        sig_sel = np.count_nonzero(y_pred_temp*y_true)\n",
    "        bkg_sel = np.count_nonzero(y_pred_temp*(1-y_true))\n",
    "        sig_rej = np.count_nonzero((1-y_pred_temp)*y_true)\n",
    "        bkg_rej = np.count_nonzero((1-y_pred_temp)*(1-y_true))\n",
    "        sig_cst.append(\n",
    "            (sig_sel / (sig_sel+bkg_sel)**0.5) if (sig_sel+bkg_sel)!=0 else 0\n",
    "        )\n",
    "    opt_cut = p_cuts[5:-5][np.array(sig_cst[5:-5])==max(sig_cst)][0]\n",
    "    y_pred_int = (y_pred > opt_cut).astype('float')\n",
    "    print(\"--> confmatrdet, obtained optimal cut is %f\" % opt_cut)\n",
    "    \n",
    "    confusionmatrix = confusion_matrix(y_true, y_pred_int, sample_weight=sample_weight, normalize=confusionmatrix_norm)\n",
    "    \n",
    "    return np.linalg.det(confusionmatrix)\n",
    "\n",
    "# balanced accuracy computed with the optimised cut, binary problem\n",
    "def custom_singlebkg_balancedaccuracy_score(\n",
    "    y_true, y_pred, *,\n",
    "    sample_weight=None,\n",
    "):  \n",
    "    \n",
    "    if not (sample_weight is None):\n",
    "        sample_weight = sample_weight.loc[y_true.index.values].values.reshape(-1)\n",
    "    \n",
    "    # compute optimal cut and use it to separate predicted signal and background\n",
    "    sig_cst = []\n",
    "    p_min = min(y_pred)\n",
    "    p_max = max(y_pred)\n",
    "    p_cuts = np.linspace(p_min, p_max, 200)\n",
    "    for p_cut in p_cuts:\n",
    "        y_pred_temp = (y_pred > p_cut).astype('float')\n",
    "        sig_sel = np.count_nonzero(y_pred_temp*y_true)\n",
    "        bkg_sel = np.count_nonzero(y_pred_temp*(1-y_true))\n",
    "        sig_rej = np.count_nonzero((1-y_pred_temp)*y_true)\n",
    "        bkg_rej = np.count_nonzero((1-y_pred_temp)*(1-y_true))\n",
    "        sig_cst.append(\n",
    "            (sig_sel / (sig_sel+bkg_sel)**0.5) if (sig_sel+bkg_sel)!=0 else 0\n",
    "        )\n",
    "    opt_cut = p_cuts[5:-5][np.array(sig_cst[5:-5])==max(sig_cst)][0]\n",
    "    y_pred_int = (y_pred > opt_cut).astype('float')\n",
    "    print(\"--> balancedaccuracy, obtained optimal cut is %f\" % opt_cut)\n",
    "    \n",
    "    # normalise weights\n",
    "    norm_weight = np.zeros(len(sample_weight))\n",
    "    norm_weight[y_true==0] = sample_weight[y_true==0]/sum(sample_weight[y_true==0])\n",
    "    norm_weight[y_true==1] = sample_weight[y_true==1]/sum(sample_weight[y_true==1])\n",
    "    \n",
    "    Ss = sum(norm_weight[(y_pred_int==1) & (y_true==1)])\n",
    "    Br = sum(norm_weight[(y_pred_int==0) & (y_true==0)])\n",
    "    \n",
    "    return (1/sum(norm_weight))*(Ss+Br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecddf5c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# classifier optimised with cross-validated scan over hyperparameter grid\n",
    "\n",
    "bClsGscv = True  # perform this analysis? if False, use previously selected hyperparameters\n",
    "bPerfPlots_save = True  # save performance plots?\n",
    "k_fold_index = 5  # nr. of folds to create from the total training set for k-folding\n",
    "\n",
    "classifier = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())  # algorithm to use\n",
    "hyperparams = {  # list of hyperparameters to scan over\n",
    "    \"base_estimator__max_depth\" : [2, 5, 10, 20],  # max_depth - maximum branch depth, set 1 for stumps\n",
    "    \"n_estimators\" : [200, 500, 1000, 2000],  # n_trees - number of trees\n",
    "    \"learning_rate\" : [1],  # alpha - AdaBoost learning rate\n",
    "    \"base_estimator__min_weight_fraction_leaf\" : [0.05],  # minimum leaf size allowed (stops splitting after that)\n",
    "}\n",
    "\n",
    "scorers = {  # list of scoring estimators\n",
    "    \"custom_singlebkg_balancedaccuracy_score\" : make_scorer(\n",
    "        custom_singlebkg_balancedaccuracy_score, sample_weight=W,\n",
    "        greater_is_better=True, needs_threshold=False, needs_proba=True,\n",
    "    ),\n",
    "    \"custom_singlebkg_hike_score\" : make_scorer(\n",
    "        custom_singlebkg_hike_score,\n",
    "        sample_weight=W, reweight=bReweightStat, sample_weight_phys=WPhys,\n",
    "        greater_is_better=True, needs_threshold=False, needs_proba=True,\n",
    "    ),\n",
    "    \"custom_singlebkg_confmatrdet_score\" : make_scorer(\n",
    "        custom_singlebkg_confmatrdet_score,\n",
    "        sample_weight=W,\n",
    "        greater_is_better=True, needs_threshold=False, needs_proba=True,\n",
    "    ),\n",
    "}\n",
    "scorer_opt = \"custom_singlebkg_hike_score\"  # choose scoring estimator wrt. which optimise the classifier\n",
    "\n",
    "if bClsGscv:\n",
    "    \n",
    "    # initialise the k-folding splitter\n",
    "    skf = StratifiedKFold(n_splits=k_fold_index)\n",
    "    print(\n",
    "        \"%d-folding will exploit training folds of %d events each\" \\\n",
    "        % (k_fold_index, y_train.shape[0]/k_fold_index)\n",
    "    )\n",
    "    \n",
    "    print(\"---\")\n",
    "    \n",
    "    # initialise & fit the scan --> find the best tree\n",
    "    cls_gscv = GridSearchCV(\n",
    "        classifier, refit=scorer_opt,\n",
    "        param_grid=hyperparams, scoring=scorers, cv=skf, return_train_score=True, verbose=4,\n",
    "    )\n",
    "    cls_gscv.fit(X_train, y_train, sample_weight=W_train)\n",
    "    hyperparams_opt = cls_gscv.best_params_\n",
    "    print(\"---\\nbest-classifier hyperparameters for %s:\" % scorer_opt)\n",
    "    print(hyperparams_opt)\n",
    "    \n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae26f2e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if bClsGscv:  # set above\n",
    "\n",
    "    # scoring comparison between different hyperparameter combinations\n",
    "    if True:\n",
    "    \n",
    "        # set the 2 hyperparameters (xplot, yplot) and the scoring estimators (zplot - list) to consider\n",
    "        xplot = \"param_base_estimator__max_depth\"\n",
    "        yplot = \"param_n_estimators\"\n",
    "        zplot = [\n",
    "            \"custom_singlebkg_balancedaccuracy_score\", \n",
    "            \"custom_singlebkg_confmatrdet_score\",\n",
    "            \"custom_singlebkg_hike_score\",\n",
    "        ]\n",
    "\n",
    "        dt_scores = pd.DataFrame(cls_gscv.cv_results_)\n",
    "        print(\"available variables:\")\n",
    "        print(dt_scores.columns)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 8), ncols=len(zplot), nrows=2)\n",
    "        for i, s in enumerate(zplot):\n",
    "            sns.heatmap(\n",
    "                dt_scores[[xplot, yplot, \"mean_train_\"+s]].pivot(\n",
    "                    index=xplot, columns=yplot, values=\"mean_train_\"+s\n",
    "                ), \n",
    "                ax=ax[0, i], linewidths=.1, edgecolors=\"k\", annot=True, \n",
    "                annot_kws={\"size\": 9}, fmt=\".3\", cmap=\"viridis\"\n",
    "            )\n",
    "            sns.heatmap(\n",
    "                dt_scores[[xplot, yplot, \"mean_test_\"+s]].pivot(\n",
    "                    index=xplot, columns=yplot, values=\"mean_test_\"+s\n",
    "                ), \n",
    "                ax=ax[1, i], linewidths=.1, edgecolors=\"k\", annot=True, \n",
    "                annot_kws={\"size\": 9}, fmt=\".3\", cmap=\"viridis\"\n",
    "            )\n",
    "            ax[0, i].set_title(\"mean_train, \"+s.replace(\"custom_\", \"\").replace(\"_score\", \"\"))\n",
    "            ax[1, i].set_title(\"mean_test, \"+s.replace(\"custom_\", \"\").replace(\"_score\", \"\"))\n",
    "        fig.tight_layout()\n",
    "\n",
    "        if bPerfPlots_save:\n",
    "            fig.savefig(\"./output_misc/cls_gscv_grids.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2b1e41",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# final BDT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380cf17",
   "metadata": {},
   "source": [
    "also saving the trained classifier to `./output_misc/bdt_ab.pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bace46",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create training/testing datasets for the new model\n",
    "X_train, X_test, y_train, y_test, W_train, W_test, WPhys_train, WPhys_test =\\\n",
    "    train_test_split(X, y, W, WPhys, test_size=split_test_fraction)\n",
    "print(\"total classification dataset size is %d\" % y.shape[0])\n",
    "print(\"training (testing) size is %d (%d)\" % (y_train.shape[0], y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520dbcb7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# single BDT with AdaBoost with selected hyperparameters\n",
    "\n",
    "bBdtAb = True  # do it?\n",
    "bPerfPlots_save = True  # save confusion matrix plots?\n",
    "bClassifier_save = True  # save the trained model?\n",
    "bManualSet = True  # set parameters manually or take those from optimisation above, if available?\n",
    "max_depth = \\\n",
    "    5 if (bManualSet | (len(hyperparams_opt)<=1)) \\\n",
    "    else hyperparams_opt[\"base_estimator__max_depth\"]  # maximum branch depth, set 1 for stumps\n",
    "n_trees = \\\n",
    "    200 if (bManualSet | (len(hyperparams_opt)<=1)) \\\n",
    "    else hyperparams_opt[\"n_estimators\"]  # number of trees\n",
    "alpha = \\\n",
    "    1 if (bManualSet | (len(hyperparams_opt)<=1)) \\\n",
    "    else hyperparams_opt[\"learning_rate\"]  # AdaBoost learning rate\n",
    "min_weight_fraction_leaf = \\\n",
    "    0.05 if (bManualSet | (len(hyperparams_opt)<=1)) \\\n",
    "    else hyperparams_opt[\"base_estimator__min_weight_fraction_leaf\"]  # minimum leaf size allowed (stops splitting after that)\n",
    "output_cut_man = 0.5  # manual set of the optimised decision cut\n",
    "bManualCut = False  # if False, output_cut_man is overridden by an automatic cut below\n",
    "bAutoCutTrain = True  # if True (False), compute automatic cut from training (testing) - useless if bManualCut is False\n",
    "\n",
    "if bBdtAb:\n",
    "    \n",
    "    # print chosen parameters\n",
    "    whichparams = \"manually defined\" if bManualSet else \"grid-borne\"\n",
    "    print(\"final BDT created with the %s parameters:\" % whichparams)\n",
    "    print(\"max_depth = %d\" % max_depth)\n",
    "    print(\"n_trees = %d\" % n_trees)\n",
    "    print(\"alpha = %f\" % alpha)\n",
    "    print(\"min_weight_fraction_leaf = %f\" % min_weight_fraction_leaf)\n",
    "\n",
    "    # initialise & fit the classifier\n",
    "    bdt_ab = AdaBoostClassifier(\n",
    "        DecisionTreeClassifier(\n",
    "            max_depth=max_depth,\n",
    "            min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "        ), \n",
    "        n_estimators=n_trees,\n",
    "        learning_rate=alpha,\n",
    "    )\n",
    "    bdt_ab.fit(X_train, y_train, sample_weight=W_train)\n",
    "    \n",
    "    # compute the class probability for each event\n",
    "    y_scores_train_float = bdt_ab.predict_proba(X_train)[:, np.where(bdt_ab.classes_==1)[0][0]]\n",
    "    y_scores_test_float = bdt_ab.predict_proba(X_test)[:, np.where(bdt_ab.classes_==1)[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c6438",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if bBdtAb:  # set above\n",
    "    \n",
    "    # performance plots, with cut at 50%\n",
    "    y_scores_train_int = (y_scores_train_float > 0.5).astype('float')\n",
    "    y_scores_test_int = (y_scores_test_float > 0.5).astype('float')\n",
    "    \n",
    "    if True:  # (float) classifier output distributions\n",
    "        bins = 100  # set nr. of bins for each distribution\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 5), ncols=2)\n",
    "        fig.suptitle(\n",
    "            \"%d trees with max. depth %d, learning rate %d, min. tot. weight per leaf %.3f; output cut %f\" \\\n",
    "            % (n_trees, max_depth, alpha, min_weight_fraction_leaf, 0.5)\n",
    "        )\n",
    "        fig.supxlabel(\"classifier output\")\n",
    "        ax[0].set_title(\"training\")\n",
    "        ax[1].set_title(\"testing\")\n",
    "        \n",
    "        for i, i_class in enumerate(df[\"class\"].unique()):\n",
    "            ax[0].hist(\n",
    "                y_scores_train_float[df[\"class\"][y_train.index]==i_class], bins=bins,\n",
    "                histtype=\"step\", label=\"true \"+[s for s in classlabel if classlabel[s]==i_class][0]\n",
    "            )\n",
    "            ax[1].hist(\n",
    "                y_scores_test_float[df[\"class\"][y_test.index]==i_class], bins=bins,\n",
    "                histtype=\"step\", label=\"true \"+[s for s in classlabel if classlabel[s]==i_class][0]\n",
    "            )\n",
    "        ax[0].axvline(0.5, ls=\"--\", color=\"k\")\n",
    "        ax[1].axvline(0.5, ls=\"--\", color=\"k\")\n",
    "        \n",
    "        ax[0].grid()\n",
    "        ax[1].grid()\n",
    "        ax[1].legend()\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        if bPerfPlots_save:\n",
    "            fig.savefig(\"./output_misc/bdt_ab_output.png\")\n",
    "    \n",
    "    if True:  # confusion matrix\n",
    "        fig, ax = plt.subplots(figsize=(12, 5), ncols=2)\n",
    "        fig.suptitle(\n",
    "            \"%d trees with max. depth %d, learning rate %d, min. tot. weight per leaf %.3f; output cut %f\" \\\n",
    "            % (n_trees, max_depth, alpha, min_weight_fraction_leaf, 0.5)\n",
    "        )\n",
    "        \n",
    "        ConfusionMatrixDisplay(\n",
    "            confusion_matrix=confusion_matrix(y_train, y_scores_train_int, normalize=confusionmatrix_norm),\n",
    "            display_labels=[whichbkg, whichsig],\n",
    "        ).plot(ax=ax[0])\n",
    "        ConfusionMatrixDisplay(\n",
    "            confusion_matrix=confusion_matrix(y_test, y_scores_test_int, normalize=confusionmatrix_norm),\n",
    "            display_labels=[whichbkg, whichsig],\n",
    "        ).plot(ax=ax[1])\n",
    "        \n",
    "        ax[0].set_title(\"training (%d events; norm. rule: %s)\" % (y_train.shape[0], confusionmatrix_norm))\n",
    "        ax[1].set_title(\"testing (%d events; norm. rule: %s)\" % (y_test.shape[0], confusionmatrix_norm))\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        if bPerfPlots_save:\n",
    "            fig.savefig(\"./output_misc/bdt_ab_confusionmatrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059678e0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if bBdtAb:  # set above\n",
    "    \n",
    "    # performance plots, at various thresholds\n",
    "    \n",
    "    # do all the necessary calculations anyway\n",
    "    p_min = min(min(y_scores_train_float), min(y_scores_test_float))\n",
    "    p_max = max(max(y_scores_train_float), max(y_scores_test_float))\n",
    "    p_cuts = np.linspace(p_min, p_max, bins)\n",
    "    \n",
    "    sig_eff_train = []\n",
    "    bkg_eff_train = []\n",
    "    sig_pur_train = []\n",
    "    sig_cst_train = []\n",
    "    \n",
    "    sig_eff_test = []\n",
    "    bkg_eff_test = []\n",
    "    sig_pur_test = []\n",
    "    sig_cst_test = []\n",
    "    \n",
    "    for p_cut in p_cuts:\n",
    "        \n",
    "        y_scores_train_temp = (y_scores_train_float > p_cut).astype('float')\n",
    "        sig_sel_train = np.count_nonzero(y_scores_train_temp*y_train)\n",
    "        bkg_sel_train = np.count_nonzero(y_scores_train_temp*(1-y_train))\n",
    "        sig_rej_train = np.count_nonzero((1-y_scores_train_temp)*y_train)\n",
    "        bkg_rej_train = np.count_nonzero((1-y_scores_train_temp)*(1-y_train))\n",
    "        sig_eff_train.append(\n",
    "            (sig_sel_train / (sig_sel_train+sig_rej_train)) if (sig_sel_train+sig_rej_train)!=0 else 0\n",
    "        )\n",
    "        bkg_eff_train.append(\n",
    "            (bkg_sel_train / (bkg_sel_train+bkg_rej_train)) if (bkg_sel_train+bkg_rej_train)!=0 else 0\n",
    "        )\n",
    "        sig_pur_train.append(\n",
    "            (sig_sel_train / (sig_sel_train+bkg_sel_train)) if (sig_sel_train+bkg_sel_train)!=0 else 0\n",
    "        )\n",
    "        sig_cst_train.append(\n",
    "            (sig_sel_train / (sig_sel_train+bkg_sel_train)**0.5) if (sig_sel_train+bkg_sel_train)!=0 else 0\n",
    "        )\n",
    "        \n",
    "        y_scores_test_temp = (y_scores_test_float > p_cut).astype('float')\n",
    "        sig_sel_test = np.count_nonzero(y_scores_test_temp*y_test)\n",
    "        bkg_sel_test = np.count_nonzero(y_scores_test_temp*(1-y_test))\n",
    "        sig_rej_test = np.count_nonzero((1-y_scores_test_temp)*y_test)\n",
    "        bkg_rej_test = np.count_nonzero((1-y_scores_test_temp)*(1-y_test))\n",
    "        sig_eff_test.append(\n",
    "            (sig_sel_test / (sig_sel_test+sig_rej_test)) if (sig_sel_test+sig_rej_test)!=0 else 0\n",
    "        )\n",
    "        bkg_eff_test.append(\n",
    "            (bkg_sel_test / (bkg_sel_test+bkg_rej_test)) if (bkg_sel_test+bkg_rej_test)!=0 else 0\n",
    "        )\n",
    "        sig_pur_test.append(\n",
    "            (sig_sel_test / (sig_sel_test+bkg_sel_test)) if (sig_sel_test+bkg_sel_test)!=0 else 0\n",
    "        )\n",
    "        sig_cst_test.append(\n",
    "            (sig_sel_test / (sig_sel_test+bkg_sel_test)**0.5) if (sig_sel_test+bkg_sel_test)!=0 else 0\n",
    "        )\n",
    "        \n",
    "    opt_cut_train = p_cuts[5:-5][np.array(sig_cst_train[5:-5])==max(sig_cst_train)][0] if\\\n",
    "                    len(p_cuts[5:-5][np.array(sig_cst_train[5:-5])==max(sig_cst_train)])==1 else\\\n",
    "                    np.mean(p_cuts[5:-5][np.array(sig_cst_train[5:-5])==max(sig_cst_train)])\n",
    "    opt_cut_test = p_cuts[5:-5][np.array(sig_cst_test[5:-5])==max(sig_cst_test)][0] if\\\n",
    "                    len(p_cuts[5:-5][np.array(sig_cst_test[5:-5])==max(sig_cst_test)])==1 else\\\n",
    "                    np.mean(p_cuts[5:-5][np.array(sig_cst_test[5:-5])==max(sig_cst_test)])\n",
    "    \n",
    "    output_cut = output_cut_man if bManualCut else (opt_cut_train if bAutoCutTrain else opt_cut_test)\n",
    "            \n",
    "    if True:  # ROC curve\n",
    "        fig, ax = plt.subplots(figsize=(6, 5))\n",
    "        fig.suptitle(\n",
    "            \"%d trees with max. depth %d, learning rate %d,\\nmin. tot. weight per leaf %.3f\" \\\n",
    "            % (n_trees, max_depth, alpha, min_weight_fraction_leaf)\n",
    "        )\n",
    "        \n",
    "        plot_roc_curve(\n",
    "            bdt_ab, X_train, y_train, sample_weight=W[W_train.index], \n",
    "            ax=ax, name=\"training\", pos_label=1\n",
    "        ) ;\n",
    "        plot_roc_curve(\n",
    "            bdt_ab, X_test, y_test, sample_weight=W[W_test.index], \n",
    "            ax=ax, name=\"testing\", pos_label=1\n",
    "        ) ;\n",
    "        ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", color=\"k\")\n",
    "        \n",
    "        ax.grid()\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        if bPerfPlots_save:\n",
    "            fig.savefig(\"./output_misc/bdt_ab_roccurve.png\")\n",
    "            \n",
    "    if True:  # performance scorers\n",
    "        bins = 200  # set nr. of cuts to probe\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 5), ncols=2)\n",
    "        fig.suptitle(\n",
    "            \"%d trees with max. depth %d, learning rate %d, min. tot. weight per leaf %.3f\" \\\n",
    "            % (n_trees, max_depth, alpha, min_weight_fraction_leaf)\n",
    "        )\n",
    "        ax[0].set_title(\"training\")\n",
    "        ax[1].set_title(\"testing\")\n",
    "            \n",
    "        ax[0].plot(p_cuts[5:-5], sig_eff_train[5:-5], color=\"C0\", label=\"Ss/(Ss+Sr) i.e. SE\")\n",
    "        ax[0].plot(p_cuts[5:-5], bkg_eff_train[5:-5], color=\"C1\", label=\"Bs/(Bs+Br)\")\n",
    "        ax[0].plot(p_cuts[5:-5], sig_pur_train[5:-5], color=\"deepskyblue\", ls=\"--\", label=\"Ss/(Ss+Bs) i.e. SP\")\n",
    "        ax[0].plot(\n",
    "            p_cuts[5:-5], np.array(sig_eff_train[5:-5])*np.array(sig_pur_train[5:-5]), \n",
    "            color=\"darkblue\", ls=\":\", lw=2, label=\"SE*SP\"\n",
    "        )\n",
    "        ax[0].plot(\n",
    "            p_cuts[5:-5], np.array(sig_cst_train[5:-5])/max(sig_cst_train), color=\"C2\", label=\"Ss/(Ss+Bs)^0.5 (norm.)\"\n",
    "        )\n",
    "        ax[1].plot(p_cuts[5:-5], sig_eff_test[5:-5], color=\"C0\", label=\"Ss/(Ss+Sr) i.e. SE\")\n",
    "        ax[1].plot(p_cuts[5:-5], bkg_eff_test[5:-5], color=\"C1\", label=\"Bs/(Bs+Br)\")\n",
    "        ax[1].plot(p_cuts[5:-5], sig_pur_test[5:-5], color=\"deepskyblue\", ls=\"--\", label=\"Ss/(Ss+Bs) i.e. SP\")\n",
    "        ax[1].plot(\n",
    "            p_cuts[5:-5], np.array(sig_eff_test[5:-5])*np.array(sig_pur_test[5:-5]), \n",
    "            color=\"darkblue\", ls=\":\", lw=2, label=\"SE*SP\"\n",
    "        )\n",
    "        ax[1].plot(\n",
    "            p_cuts[5:-5], np.array(sig_cst_test[5:-5])/max(sig_cst_test), color=\"C2\", label=\"Ss/(Ss+Bs)^0.5 (norm.)\"\n",
    "        )\n",
    "        \n",
    "        ax[0].axvline(opt_cut_train, lw=1, color=\"k\", ls=\"--\", label=\"optimal cut\")\n",
    "        ax[1].axvline(opt_cut_train, lw=1, color=\"k\", ls=\"--\", label=\"optimal cut\")\n",
    "        \n",
    "        ax[0].legend()\n",
    "        ax[0].grid()\n",
    "        ax[1].grid()\n",
    "        \n",
    "        fig.supylabel(\"performance estimator\")\n",
    "        fig.supxlabel(\"cut on classifier output\")\n",
    "        fig.tight_layout()\n",
    "        if bPerfPlots_save:\n",
    "            fig.savefig(\"./output_misc/bdt_ab_cuttrends.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da692b87",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if bBdtAb:  # set above\n",
    "    \n",
    "    # performance plots, after cut optimisation\n",
    "    y_scores_train_int = (y_scores_train_float > output_cut).astype('float')\n",
    "    y_scores_test_int = (y_scores_test_float > output_cut).astype('float')\n",
    "    \n",
    "    if True:  # (float) classifier output distributions\n",
    "        bins = 100  # set nr. of bins for each distribution\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 5), ncols=2)\n",
    "        fig.suptitle(\n",
    "            \"%d trees with max. depth %d, learning rate %d, min. tot. weight per leaf %.3f; output cut %f\" \\\n",
    "            % (n_trees, max_depth, alpha, min_weight_fraction_leaf, output_cut)\n",
    "        )\n",
    "        fig.supxlabel(\"classifier output\")\n",
    "        ax[0].set_title(\"training\")\n",
    "        ax[1].set_title(\"testing\")\n",
    "        \n",
    "        for i, i_class in enumerate(df[\"class\"].unique()):\n",
    "            ax[0].hist(\n",
    "                y_scores_train_float[df[\"class\"][y_train.index]==i_class], bins=bins,\n",
    "                histtype=\"step\", label=\"true \"+[s for s in classlabel if classlabel[s]==i_class][0]\n",
    "            )\n",
    "            ax[1].hist(\n",
    "                y_scores_test_float[df[\"class\"][y_test.index]==i_class], bins=bins,\n",
    "                histtype=\"step\", label=\"true \"+[s for s in classlabel if classlabel[s]==i_class][0]\n",
    "            )\n",
    "        ax[0].axvline(output_cut, ls=\"--\", color=\"k\", label=\"optimal cut\")\n",
    "        ax[1].axvline(output_cut, ls=\"--\", color=\"k\", label=\"optimal cut\")\n",
    "        \n",
    "        ax[0].grid()\n",
    "        ax[1].grid()\n",
    "        ax[1].legend()\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        if bPerfPlots_save:\n",
    "            fig.savefig(\"./output_misc/bdt_ab_outputopt.png\")\n",
    "    \n",
    "    if True:  # confusion matrix\n",
    "        fig, ax = plt.subplots(figsize=(12, 5), ncols=2)\n",
    "        fig.suptitle(\n",
    "            \"%d trees with max. depth %d, learning rate %d, min. tot. weight per leaf %.3f; output cut %f\" \\\n",
    "            % (n_trees, max_depth, alpha, min_weight_fraction_leaf, output_cut)\n",
    "        )\n",
    "        \n",
    "        ConfusionMatrixDisplay(\n",
    "            confusion_matrix=confusion_matrix(y_train, y_scores_train_int, normalize=confusionmatrix_norm),\n",
    "            display_labels=[whichbkg, whichsig],\n",
    "        ).plot(ax=ax[0])\n",
    "        ConfusionMatrixDisplay(\n",
    "            confusion_matrix=confusion_matrix(y_test, y_scores_test_int, normalize=confusionmatrix_norm),\n",
    "            display_labels=[whichbkg, whichsig],\n",
    "        ).plot(ax=ax[1])\n",
    "        \n",
    "        ax[0].set_title(\"training (%d events; norm. rule: %s)\" % (y_train.shape[0], confusionmatrix_norm))\n",
    "        ax[1].set_title(\"testing (%d events; norm. rule: %s)\" % (y_test.shape[0], confusionmatrix_norm))\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        if bPerfPlots_save:\n",
    "            fig.savefig(\"./output_misc/bdt_ab_confusionmatrixopt.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8f47ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# save Pickle file with classifier\n",
    "\n",
    "# set notes to be saved in the model manually here:\n",
    "notes = \"manually selected parameters (which maximise accuracy)\"\n",
    "\n",
    "if bBdtAb & bClassifier_save:  # set above\n",
    "    cls = {\n",
    "        \"classifier\" : bdt_ab,\n",
    "        \"parameter_choice\" : \"manual\" if bManualSet else \"grid-borne\",\n",
    "        \"output_cut\" : {\n",
    "            \"training\" : opt_cut_train,\n",
    "            \"testing\" : opt_cut_test,\n",
    "            \"manual\" : output_cut_man,\n",
    "            \"used_for_evaluation\" : \"manual\" if bManualCut else (\"training\" if bAutoCutTrain else \"testing\")\n",
    "        },\n",
    "        \"variables\" : list(X.columns),\n",
    "        \"weight_rule\" : \"physical\" if bReweightStat else \"normalised\",\n",
    "        \"weight_range_lambda\" : limLambdaWeights,\n",
    "        \"signal\" : {\n",
    "            \"class\" : whichsig,\n",
    "            \"training_population\" : y_train[y==1].shape[0],\n",
    "            \"testing_population\" : y_test[y==1].shape[0],\n",
    "        },\n",
    "        \"background\" : {\n",
    "            \"class\" : whichbkg,\n",
    "            \"training_population\" : y_train[y==0].shape[0],\n",
    "            \"testing_population\" : y_test[y==0].shape[0],\n",
    "        },\n",
    "        \"notes\" : notes,\n",
    "    }, \n",
    "    pickle.dump(cls, open(\"./output_misc/bdt_ab.pickle\", \"wb\"))\n",
    "    print(\"output pickle file saved with note:\")\n",
    "    print(notes)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
